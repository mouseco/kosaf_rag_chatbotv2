"""
ê´€ë¦¬ììš© PDF ì¼ê´„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (v2: ì„±ëŠ¥ ë° UX ê°œì„ )

ê°œì„ ì :
- ì„ë² ë”© ì €ì¥ ì‹œ ë°°ì¹˜(Batch) ì‚½ì…ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ê·¹ëŒ€í™”
- 'tqdm' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹œê°ì ì¸ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì œê³µ
- 'rich' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë” ê¹”ë”í•˜ê³  ê°€ë…ì„± ì¢‹ì€ ì½˜ì†” ì¶œë ¥
"""

import os
import warnings
import uuid
from pathlib import Path
import google.generativeai as genai
import re
from langchain_text_splitters import RecursiveCharacterTextSplitter
import pdfplumber
from dotenv import load_dotenv
from supabase import create_client, Client
from typing import List ,Dict
from tqdm import tqdm  
from rich import print  

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¤ì •
PDF_DIRECTORY = "pdf_files_to_upload"
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")  # service_role í‚¤ ì‚¬ìš©
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# í™˜ê²½ë³€ìˆ˜ ê²€ì¦
if not all([SUPABASE_URL, SUPABASE_SERVICE_KEY, GOOGLE_API_KEY]):
    print("[bold red]âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.[/bold red]")
    print("í•„ìš”í•œ ë³€ìˆ˜: SUPABASE_URL, SUPABASE_SERVICE_KEY, GOOGLE_API_KEY")
    exit(1)

# íƒ€ì… ê²€ì¦ì„ ìœ„í•œ assert ì¶”ê°€
assert SUPABASE_URL is not None
assert SUPABASE_SERVICE_KEY is not None  
assert GOOGLE_API_KEY is not None

def extract_text_from_pdf(file_path: str) -> str:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        extracted_text = ""
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        extracted_text += f"\n\n--- í˜ì´ì§€ {page_num} ---\n\n"
                        extracted_text += page_text
        return extracted_text.strip()
    except Exception as e:
        raise Exception(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")



# [êµì²´] ìƒˆë¡œìš´ ì²­í‚¹ í•¨ìˆ˜
def split_legal_text_into_chunks(text: str, max_chunk_size: int = 2000) -> List[Dict]:
    """
    ì†Œì œëª© ë‹¨ìœ„ë¡œ ì²­í‚¹í•˜ëŠ” ê°œì„  ë²„ì „.
    - ì†Œì œëª© ì•„ë˜ ëª¨ë“  Q&Aë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ìŒ.
    - ì²­í¬ê°€ ê¸¸ë©´ ì¬ê·€ ë¶„í•  (ì˜¤ë²„ë© 200ì).
    - ë°˜í™˜: [{'content': str, 'metadata': dict}, ...]
    """
    
    # 1. ì†Œì œëª© íŒ¨í„´ ì •ì˜ (ê°•í™”ë¨)
    section_patterns = [
        r'^(\d+\.\s*[ê°€-í£\sÂ·,]+)$',  # "1. ê³µí†µì‚¬í•­"
        r'^([ê°€-í£\sÂ·,]+?(?:ë¶€ì„œ\s*ê°„|ì‚¬ì—…|ì˜ˆì‚°|ê²½ë¹„|ê³µí†µ|ê´€ë¦¬|ì—…ë¬´|ê³„ì•½|ì§‘í–‰|ì¸ì‚¬|ì œë„|ë²•ë¥ )[^\n]*?)$',  # í‚¤ì›Œë“œ í¬í•¨ ì†Œì œëª©
    ]
    
    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì†Œì œëª© ìœ„ì¹˜ ì°¾ê¸° (re.finditerë¡œ ìœ„ì¹˜ ìº¡ì²˜)
    section_matches = []
    for pat in section_patterns:
        matches = list(re.finditer(pat, text, re.MULTILINE))
        section_matches.extend(matches)
    section_matches.sort(key=lambda m: m.start())  # ì‹œì‘ ìœ„ì¹˜ ìˆœ ì •ë ¬
    
    # 2. ì†Œì œëª©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í•  (ì„¹ì…˜ ê°„ ê²½ê³„)
    sections = []
    prev_end = 0
    for match in section_matches:
        section_title = match.group(1).strip()
        section_start = match.start()
        section_end = match.end()
        
        # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ì „ê¹Œì§€ì˜ ë‚´ìš© (Q&A í¬í•¨)
        next_start = section_matches[section_matches.index(match) + 1].start() if section_matches.index(match) + 1 < len(section_matches) else len(text)
        section_content = text[prev_end:next_start].strip()
        
        sections.append({
            'title': section_title,
            'content': section_content,
            'start_pos': prev_end,
            'end_pos': next_start
        })
        prev_end = next_start
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ (fallback)
    if prev_end < len(text):
        sections.append({
            'title': 'ê¸°íƒ€',
            'content': text[prev_end:].strip(),
            'start_pos': prev_end,
            'end_pos': len(text)
        })
    
    # 3. ê° ì„¹ì…˜ ë‚´ Q&A ê·¸ë£¹í™” ë° ì²­í¬ ìƒì„±
    structured_chunks = []
    for sec in sections:
        section_text = sec['content']
        if not section_text:
            continue
        
        # ì„¹ì…˜ ë‚´ Q&A íŒ¨í„´ ë§¤ì¹˜ (ì „ì²´ ì„¹ì…˜ ìŠ¤ìº”)
        qa_pattern = r"(Q: [^\?]+\?)\s*(A: .+?)(?=(Q: |$))"
        qa_matches = list(re.finditer(qa_pattern, section_text, re.DOTALL | re.MULTILINE))
        
        if not qa_matches:
            # Q&A ì—†ìœ¼ë©´ ì„¹ì…˜ ì „ì²´ë¥¼ ì²­í¬ë¡œ
            chunk_content = section_text
            qa_list = []
        else:
            # Q&Aë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
            qa_contents = []
            qa_list = []
            for match in qa_matches:
                question = match.group(1).strip()
                answer = match.group(2).strip()
                qa_contents.append(f"{question}\n{answer}")
                qa_list.append({'question': question, 'answer': answer})
            chunk_content = "\n\n".join(qa_contents)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        metadata = {
            'category': sec['title'],
            'qa_list': qa_list,  # ì „ì²´ Q&A ëª©ë¡ (ê²€ìƒ‰ ì‹œ ìƒì„¸ ì°¸ì¡°)
            'section_start': sec['start_pos'],
            'section_end': sec['end_pos']
        }
        
        structured_chunks.append({
            'content': chunk_content,
            'metadata': metadata
        })
    
    # 4. ì¬ê·€ì  ë¶„í•  (ê¸´ ì²­í¬ë§Œ, max_chunk_size=2000ìœ¼ë¡œ ì¡°ì •)
    final_chunks = []
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_chunk_size,
        chunk_overlap=200,  # ì˜¤ë²„ë© ì¦ê°€ë¡œ ë§¥ë½ ìœ ì§€
        separators=["\n\n--- Q&A --- \n\n", "\n\n", "\n", " ", ""]  # Q&A ê²½ê³„ ìš°ì„ 
    )
    
    for struct_chunk in structured_chunks:
        content = struct_chunk['content']
        metadata = struct_chunk['metadata']
        if len(content) <= max_chunk_size:
            final_chunks.append({'content': content, 'metadata': metadata})
        else:
            sub_chunks = recursive_splitter.split_text(content)
            for j, sub_content in enumerate(sub_chunks):
                sub_meta = metadata.copy()
                sub_meta['sub_chunk_index'] = j
                if j > 0:
                    sub_content = f"{metadata['category']} (ê³„ì† {j})\n{sub_content}"
                final_chunks.append({'content': sub_content, 'metadata': sub_meta})
    
    return [chunk for chunk in final_chunks if chunk.get('content', '').strip()]

def process_single_pdf(file_path: str, filename: str, supabase: Client) -> bool:
    """ë‹¨ì¼ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë°°ì¹˜ ì²˜ë¦¬ ì ìš©)"""
    try:
        print(f"\n--- ğŸ“„ [bold cyan]'{filename}'[/bold cyan] ì²˜ë¦¬ ì‹œì‘ ---")
        
        # 1. ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ì‚­ì œ (ë©±ë“±ì„± ë³´ì¥)
        print("   âœ“ ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘...")
        existing_doc = supabase.table('documents').select('id, file_path').eq('title', filename).execute()
        
        if existing_doc.data:
            doc_id = existing_doc.data[0]['id']
            old_file_path = existing_doc.data[0].get('file_path')
            
            print(f"   âš ï¸  ê¸°ì¡´ '{filename}' ë°œê²¬ - ë®ì–´ì“°ê¸° ì§„í–‰")
            
            # ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ
            supabase.table('document_embeddings').delete().eq('document_id', doc_id).execute()
            
            # ê¸°ì¡´ Storage íŒŒì¼ ì‚­ì œ
            if old_file_path:
                try:
                    supabase.storage.from_('documents').remove([old_file_path])
                except:
                    pass  # Storage ì‚­ì œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
            
            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
            supabase.table('documents').delete().eq('id', doc_id).execute()
            print("   âœ“ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")

        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("   ğŸ“ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        content = extract_text_from_pdf(file_path)
        
        if len(content.strip()) < 10:
            print("   âŒ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ - ê±´ë„ˆëœ€")
            return False

        print(f"   âœ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ([bold green]{len(content):,}[/bold green]ì)")

        # [ìˆ˜ì •] 3. Storageì— ì €ì¥í•  ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        print("   ï¿½ Storageì— ì €ì¥í•  ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„± ì¤‘...")
        file_extension = Path(filename).suffix  # ì›ë³¸ íŒŒì¼ì˜ í™•ì¥ì (ì˜ˆ: '.pdf')
        safe_filename = f"{uuid.uuid4()}{file_extension}"
        storage_path = f"public/{safe_filename}"
        print(f"   âœ“ ìƒì„±ëœ ê²½ë¡œ: [dim]{storage_path}[/dim]")

        # 4. Storageì— ì›ë³¸ íŒŒì¼ ì—…ë¡œë“œ (ìƒì„±ëœ safe_filename ì‚¬ìš©)
        print("   ğŸ“ Storageì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
        with open(file_path, 'rb') as f:
            supabase.storage.from_('documents').upload(
                path=storage_path,
                file=f,
                file_options={"upsert": "true", "content-type": "application/pdf"}
            )
        
        print("   âœ“ Storage ì—…ë¡œë“œ ì™„ë£Œ")

        # [ìˆ˜ì •] 5. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ (titleê³¼ file_path ë¶„ë¦¬)
        print("   ğŸ’¾ ë¬¸ì„œ ì •ë³´ DB ì €ì¥ ì¤‘...")
        doc_result = supabase.table('documents').insert({
            'title': filename,          # í‘œì‹œìš© ì´ë¦„: ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©
            'content': content,
            'file_path': storage_path,  # ì €ì¥ìš© ê²½ë¡œ: ìƒˆë¡œ ìƒì„±í•œ ì•ˆì „í•œ ê²½ë¡œ ì‚¬ìš©
            'is_public': True,  # ëª¨ë“  ë¬¸ì„œë¥¼ ê³µìš©ìœ¼ë¡œ ì„¤ì •
            # user_id í•„ë“œ ì œê±° (NULLë¡œ ì„¤ì •ë¨)
        }).execute()
        
        if not doc_result.data:
            raise Exception("ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨")
        
        document_id = doc_result.data[0]['id']
        print("   âœ“ ë¬¸ì„œ ì •ë³´ ì €ì¥ ì™„ë£Œ")

        # 5. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
        print("   âœ‚ï¸  í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ì¤‘...")
        chunks = split_legal_text_into_chunks(content)
        chunks = [chunk for chunk in chunks if chunk.get('content', '').strip()]
        
        if not chunks:
            print("   âŒ ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        print(f"   âœ“ [bold green]{len(chunks)}[/bold green]ê°œ ì²­í¬ ìƒì„±")

        # [ìˆ˜ì •] 6. ì„ë² ë”© ìƒì„± ë° ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
        print("   ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘ (ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„)...")
        embeddings_to_insert = []
        
        # tqdmì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìƒì„±
        for i, chunk_dict in enumerate(tqdm(chunks, desc="      ìƒì„± ì§„í–‰ë¥ ", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}")):
            try:
                chunk_content = chunk_dict['content']
                embedding_result = genai.embed_content(  # type: ignore
                    model="models/text-embedding-004",
                    content=chunk_content,
                    task_type="RETRIEVAL_DOCUMENT"
                )
                embedding = embedding_result['embedding']
                
                # DBì— ë°”ë¡œ insertí•˜ì§€ ì•Šê³ , ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                embeddings_to_insert.append({
                    'document_id': document_id,
                    'chunk_index': i,
                    'content': chunk_content,
                    'embedding': embedding
                })
            except Exception as chunk_error:
                print(f"   [yellow]âš ï¸ ì²­í¬ {i+1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(chunk_error)[:100]}...[/yellow]")
                continue
        
        # [ìˆ˜ì •] 7. ì¤€ë¹„ëœ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ë¥¼ DBì— í•œ ë²ˆì— ì €ì¥
        if embeddings_to_insert:
            print(f"   ğŸš€ [bold green]{len(embeddings_to_insert)}[/bold green]ê°œ ì„ë² ë”©ì„ DBì— ì¼ê´„ ì €ì¥ ì¤‘...")
            supabase.table('document_embeddings').insert(embeddings_to_insert).execute()
            print(f"   ğŸ‰ [bold green]'{filename}'[/bold green] ì²˜ë¦¬ ì™„ë£Œ!")
            return True
        else:
            print(f"   âŒ [bold red]'{filename}' ì²˜ë¦¬ ì‹¤íŒ¨ - ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.[/bold red]")
            return False
            
    except Exception as e:
        print(f"   âŒ [bold red]'{filename}' ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}[/bold red]")
        # ì‹¤íŒ¨ ì‹œ ìƒì„±ëœ ë¬¸ì„œ ì •ë³´ ë¡¤ë°± (ì„ íƒì )
        if 'document_id' in locals():
            try:
                supabase.table('documents').delete().eq('id', document_id).execute()
            except:
                pass
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("[bold green]ğŸš€ PDF ì¼ê´„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (v2)[/bold green]")
    print("=" * 60)
    
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    print("ğŸ”§ Supabase ë° Google API ì´ˆê¸°í™” ì¤‘...")
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY) # type: ignore
        genai.configure(api_key=GOOGLE_API_KEY)  # type: ignore
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # PDF í´ë” í™•ì¸
    pdf_dir = Path(PDF_DIRECTORY)
    if not pdf_dir.exists():
        print(f"ğŸ“ '{PDF_DIRECTORY}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        pdf_dir.mkdir(exist_ok=True)
        print(f"ğŸ“‹ '{PDF_DIRECTORY}' í´ë”ì— PDF íŒŒì¼ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    pdf_files = [f for f in pdf_dir.iterdir() if f.suffix.lower() == '.pdf']
    
    if not pdf_files:
        print(f"ğŸ“‚ '{PDF_DIRECTORY}' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ“‹ ì²˜ë¦¬í•  PDF íŒŒì¼ë“¤ì„ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")
        return

    print(f"ğŸ¯ ì´ [bold yellow]{len(pdf_files)}[/bold yellow]ê°œì˜ PDF íŒŒì¼ ë°œê²¬")
    print("ğŸ“‹ íŒŒì¼ ëª©ë¡:")
    for i, file_path in enumerate(pdf_files, 1):
        file_size = file_path.stat().st_size / (1024 * 1024)  # MBë¡œ ë³€í™˜
        print(f"  {i:2d}. {file_path.name} ([cyan]{file_size:.1f}MB[/cyan])")
    
    print("\n" + "=" * 60)
    
    # ì²˜ë¦¬ ì‹œì‘
    successful_files = 0
    failed_files = []
    
    for file_path in pdf_files:
        success = process_single_pdf(str(file_path), file_path.name, supabase)
        if success:
            successful_files += 1
        else:
            failed_files.append(file_path.name)

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("[bold green]ğŸ ëª¨ë“  ì‘ì—… ì™„ë£Œ![/bold green]")
    print(f"   - âœ… [bold green]ì„±ê³µ: {successful_files}ê°œ[/bold green]")
    print(f"   - âŒ [bold red]ì‹¤íŒ¨: {len(failed_files)}ê°œ[/bold red]")
    
    if failed_files:
        print("\nğŸ” ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for filename in failed_files:
            print(f"  - [red]{filename}[/red]")
        print("\nğŸ’¡ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("  - PDFê°€ ì†ìƒë˜ì—ˆê±°ë‚˜ ì•”í˜¸ë¡œ ë³´í˜¸ëœ íŒŒì¼ì¸ì§€ í™•ì¸")
        print("  - í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDFì¸ì§€ í™•ì¸ (ìŠ¤ìº”ë³¸ì€ OCR í•„ìš”)")
    
    print(f"\nğŸ‰ ì—…ë¡œë“œëœ [bold green]{successful_files}[/bold green]ê°œ íŒŒì¼ì€ Streamlit ì•±ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()